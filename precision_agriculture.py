# -*- coding: utf-8 -*-
"""Precision_Agriculture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zADXkZRml4361uyYGYb-iySf2jeOfUbG

*   NumPy : used to perform a wide variety of mathematical operations on arrays
*   Pandas : open source Python package that is most widely used for data science/data analysis and machine learning tasks
*   Seaborn : data visualization library for statistical graphics plotting in Python.
*   Pickle : primarily used in serializing and deserializing a Python object structure
*   MatplotLib : plotting library used for 2D graphics in python programming language
*   Sklearn : contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import pickle
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.ensemble import ExtraTreesRegressor

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""**dataset_path** is path from **Google Drive** to **dataset of crop_recommendation**"""

# Path needs to be changed as per location of dataset in mounted drive

dataset_path = '/content/drive/MyDrive/Cloud/Crop_recommendation.csv'
df = pd.read_csv(dataset_path)

"""1.   Shows Dataset of given size (default = 5)
2.   Shape of entire dataset (Rows,Columns)
3.   All features available in dataset
4.   Different Types of Crops that can be recommended





"""

sample_size = 5
print("\nDataset Sample: ")
print(df.head(sample_size))
print("\nShape of Dataset: ")
print(df.shape)
print("\nFeatures of Dataset: ")
print(df.columns)
print("\nDifferent Crop Labels: ")
print(df['label'].unique())

"""Number of null fields for each features"""

df.isnull().sum()

"""Dataset description summary"""

df.describe()

"""Encoding Labels in dataset"""

# label_crop_code = df["label"].astype('category').tolist()
# replace_map_comp = {'label' : {k: v for k,v in zip(label_crop_code,list(range(1,len(label_crop_code)+1)))}}
# print("Encoded Crop Code: \n")
# print(replace_map_comp)
final_data_set = df.copy()
# final_data_set.replace(replace_map_comp, inplace=True)

"""Sample of Processed Dataset"""

print("Processed Dataset Sample: ")
final_data_set.head(5)

"""Matrix of Correlation between different features and label of dataset"""

final_data_set.corr()

"""Plot pairwise relationships in a dataset."""

sns.pairplot(final_data_set)

"""Plot correlation rectangular data as a color-encoded matrix"""

corrmat = final_data_set.corr()
top_corr_features = corrmat.index
plt.figure(figsize = (20,20))
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn", linewidths=.5)

"""Seperate Features and label"""

X = final_data_set[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]
y = final_data_set['label']

label_crop_code = df["label"].astype('category').tolist()
replace_map_comp = {'label' : {k: v for k,v in zip(label_crop_code,list(range(1,len(label_crop_code)+1)))}}
correlation_data_set = df.copy()
correlation_data_set.replace(replace_map_comp, inplace=True)

corr_X = correlation_data_set[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]
corr_y = correlation_data_set['label']

model = ExtraTreesRegressor()
model.fit(corr_X,corr_y)

feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(len(corr_X)).plot(kind='barh')
plt.title('Feature Correlation to Label')
plt.xlabel('Correlation')
plt.ylabel('Feature')
plt.show()

"""Feature Sample Dataset"""

print("Feature Dataset Sample: ")
X.head(5)

"""

Spliting available data as Train and Test

"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

# Initialzing empty lists to append all model's name and corresponding name
acc = []
model = []

"""## Decision Tree"""

DecisionTree = DecisionTreeClassifier(criterion="entropy",random_state=2,max_depth=5)

DecisionTree.fit(X,y)

predicted_values = DecisionTree.predict(X_test)
x = metrics.accuracy_score(y_test, predicted_values)
acc.append(x)
model.append('Decision Tree')
print("DecisionTrees's Accuracy is: ", x*100)

print(classification_report(y_test,predicted_values))

score = cross_val_score(DecisionTree,  X, y,cv=5)
print(score)

# Dump the trained Naive Bayes classifier with Pickle
DT_pkl_filename = '/content/drive/MyDrive/Cloud/Models/DecisionTree.pkl'
# Open the file to save as pkl file
DT_Model_pkl = open(DT_pkl_filename, 'wb')
pickle.dump(DecisionTree, DT_Model_pkl)
# Close the pickle instances
DT_Model_pkl.close()

"""## Naive Bayes"""

NaiveBayes = GaussianNB()

NaiveBayes.fit(X_train,y_train)

predicted_values = NaiveBayes.predict(X_test)
x = metrics.accuracy_score(y_test, predicted_values)
acc.append(x)
model.append('Naive Bayes')
print("Naive Bayes's Accuracy is: ", x)

print(classification_report(y_test,predicted_values))

score = cross_val_score(NaiveBayes,X,y,cv=5)
score

# Dump the trained Naive Bayes classifier with Pickle
NB_pkl_filename = '/content/drive/MyDrive/Cloud/Models/NBClassifier.pkl'
# Open the file to save as pkl file
NB_Model_pkl = open(NB_pkl_filename, 'wb')
pickle.dump(NaiveBayes, NB_Model_pkl)
# Close the pickle instances
NB_Model_pkl.close()

"""## SVM"""

SVM = SVC(gamma='auto')

SVM.fit(X_train,y_train)

predicted_values = SVM.predict(X_test)

x = metrics.accuracy_score(y_test, predicted_values)
acc.append(x)
model.append('SVM')
print("SVM's Accuracy is: ", x)

print(classification_report(y_test,predicted_values))

# Cross validation score (SVM)
score = cross_val_score(SVM,X,y,cv=5)
score

# Dump the trained Naive Bayes classifier with Pickle
SVM_pkl_filename = '/content/drive/MyDrive/Cloud/Models/SVM.pkl'
# Open the file to save as pkl file
SVM_Model_pkl = open(SVM_pkl_filename, 'wb')
pickle.dump(SVM, SVM_Model_pkl)
# Close the pickle instances
SVM_Model_pkl.close()

"""## Logistic Regression"""

LogReg = LogisticRegression(random_state=2)

LogReg.fit(X_train,y_train)

predicted_values = LogReg.predict(X_test)

x = metrics.accuracy_score(y_test, predicted_values)
acc.append(x)
model.append('Logistic Regression')
print("Logistic Regression's Accuracy is: ", x)

print(classification_report(y_test,predicted_values))

# Cross validation score (Logistic Regression)
score = cross_val_score(LogReg,X,y,cv=5)
score

plt.figure(figsize=[10,5],dpi = 100)
plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')
sns.barplot(x = acc,y = model,palette='dark')

data = np.array([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]])
prediction = NaiveBayes.predict(data)
print(prediction)